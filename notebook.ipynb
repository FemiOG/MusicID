{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4905db9-005f-42f0-aa6b-1408acef7371",
    "_uuid": "4c065a37dd33e869d93ccd8d78daed628e58112b"
   },
   "source": [
    "<a id=\"loading_data\"></a>\n",
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = {\n",
    "        \"applicationId\": \"LSETF/ME/0000045444\",\n",
    "        \"scheme\": 2,\n",
    "        \"lcda_lga\": \"Alimosho\",\n",
    "        \"loan_amount_requested\": 500000,\n",
    "        \"subsector\": \"TAILORING MATERIALS/ TRAINING ON BEADS\",\n",
    "        \"annualturnover\": 3600000,\n",
    "        \"noofemployees\": 2,\n",
    "        \"employmentcapacityscore\": 0,\n",
    "        \"totalscore\": 0,\n",
    "        \"currentturnover\": 0,\n",
    "        \"loanpurpose\": 0,\n",
    "        \"monthlyTurnOver\": 300000,\n",
    "        \"monthlySavings\": 100000,\n",
    "        \"monthlyExpenses\": 200000,\n",
    "        \"fullTimeEmployee\": 1,\n",
    "        \"contractEmployee\": 0,\n",
    "        \"totalExpectedEmployee\": 0,\n",
    "        \"onlyIncomeSource\": True,\n",
    "        \"businessknowledge\": 0,\n",
    "        \"loanassessmentstatus\": 0,\n",
    "        \"currentmonthlysav\": 100000,\n",
    "        \"age\": 33,\n",
    "        \"maritalStatus\": \"Single\",\n",
    "        \"gender\": \"Female\",\n",
    "        \"residentialStatus\": \"Rented\",\n",
    "        \"educationLevel\": \"VocationalTraining\",\n",
    "        \"timeAtAddress\": \"3+ years\",\n",
    "        \"expectedMonthlyTurnover\": 500000,\n",
    "        \"expectedMonthlyExpenses\": 300000,\n",
    "        \"expectedMonthlySavings\": 200000,\n",
    "        \"expectedFulltimeEmployee\": 2,\n",
    "        \"expectedContractEmployee\": 1,\n",
    "        \"businessType\": \"\",\n",
    "        \"sector\": \"Other\"\n",
    "    }\n",
    "normalized_variable_names = {\"lcda_lga\":\"lcda/lga\", \"loan_amount_requested\":\"loan amount requested\",\"sector\":\"selectedsector\",\n",
    "                             \"monthlySavings\":\"currentmonthlysaving\",\"expectedMonthlySavings\":\"projectedmonthlysaving\",\"businessknowledge\":\"businessknowledgescore\",\n",
    "                             \"loanpurpose\":\"loanpurposescore\",\"expectedMonthlySavings\":\"projectedmonthlysav\"}\n",
    "\n",
    "data_features = list(data_features.keys())                            \n",
    "\n",
    "model_features =['scheme','lcda/lga','loan amount requested','total affordable loan amount',\n",
    "      'max. loan amount recommended','selectedsector','credithistorystatus','annualturnover','noofemployees',\n",
    "     'employmentcapacity','educationscore','totalscore','currentturnoverscore','employmentcapacityscore','loanpurposescore',\n",
    "     'vsmscore','currentmonthlysaving','projectedmonthlysaving','apms','macli','rexperiencescore','businessknowledgescore',\n",
    "     'trackrecordscore','loanassessmentstatus','currentmonthlysav','projectedmonthlysav','gender','age','minob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applicationId',\n",
       " 'scheme',\n",
       " 'lcda/lga',\n",
       " 'loan amount requested',\n",
       " 'subsector',\n",
       " 'annualturnover',\n",
       " 'noofemployees',\n",
       " 'employmentcapacityscore',\n",
       " 'totalscore',\n",
       " 'currentturnover',\n",
       " 'loanpurposescore',\n",
       " 'monthlyTurnOver',\n",
       " 'currentmonthlysaving',\n",
       " 'monthlyExpenses',\n",
       " 'fullTimeEmployee',\n",
       " 'contractEmployee',\n",
       " 'totalExpectedEmployee',\n",
       " 'onlyIncomeSource',\n",
       " 'businessknowledgescore',\n",
       " 'loanassessmentstatus',\n",
       " 'currentmonthlysav',\n",
       " 'age',\n",
       " 'maritalStatus',\n",
       " 'gender',\n",
       " 'residentialStatus',\n",
       " 'educationLevel',\n",
       " 'timeAtAddress',\n",
       " 'expectedMonthlyTurnover',\n",
       " 'expectedMonthlyExpenses',\n",
       " 'projectedmonthlysav',\n",
       " 'expectedFulltimeEmployee',\n",
       " 'expectedContractEmployee',\n",
       " 'businessType',\n",
       " 'selectedsector']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_features:\n",
    "    if i in list(normalized_variable_names.keys()):\n",
    "        data_features[data_features.index(i)] = normalized_variable_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 29\n"
     ]
    }
   ],
   "source": [
    "print (len(data_features),len(model_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trackrecordscore', 'rexperiencescore', 'vsmscore', 'credithistorystatus', 'apms', 'macli', 'max. loan amount recommended', 'minob', 'employmentcapacity', 'projectedmonthlysaving', 'educationscore', 'currentturnoverscore', 'total affordable loan amount'}\n"
     ]
    }
   ],
   "source": [
    "print ((set(model_features).difference(set(data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maritalStatus', 'applicationId', 'currentturnover', 'totalExpectedEmployee', 'fullTimeEmployee', 'monthlyExpenses', 'residentialStatus', 'monthlyTurnOver', 'timeAtAddress', 'expectedFulltimeEmployee', 'educationLevel', 'expectedContractEmployee', 'subsector', 'onlyIncomeSource', 'expectedMonthlyExpenses', 'businessType', 'expectedMonthlyTurnover', 'contractEmployee'}\n"
     ]
    }
   ],
   "source": [
    "print ((set(data_features).difference(set(model_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5abea3ac-4fa5-4c4f-893f-7f2afa49e523",
    "_kg_hide-output": true,
    "_uuid": "337e0950ca948be32d5d881c1a3c675ccf7ac523"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1001)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm_notebook\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "97700e3e-82e1-4ce2-9da4-3f8f264e7558",
    "_kg_hide-output": true,
    "_uuid": "2ca1929548de57afb1c4fde19c10f7b18c64264e"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a418ce4d-b104-4710-b50d-e9ab1e7e420f",
    "_kg_hide-output": true,
    "_uuid": "1acc16aa65e8f39a5abd8b60906740a671659f1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>artist_song</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RaLMYYE7HuG6i67bQ2t3cn</td>\n",
       "      <td>king sunny adeSynchro System.mp3</td>\n",
       "      <td>King Sunny Ade</td>\n",
       "      <td>Juju</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D7nFHkDRet5sMJ9LSsQEdf</td>\n",
       "      <td>nHfXJu68Yq28bT7oJbW7gA.mp3</td>\n",
       "      <td>King Sunny Ade</td>\n",
       "      <td>Juju</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>zyMmRtsknzzNhFaB7kohtD</td>\n",
       "      <td>king Sunny Ade  Asiko Layee  Eda Kan Ko  mola ...</td>\n",
       "      <td>King Sunny Ade</td>\n",
       "      <td>Juju</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PHtSce72gTmGC3VHAgL7hY</td>\n",
       "      <td>King Sunny Ade - Penkele.mp3</td>\n",
       "      <td>King Sunny Ade</td>\n",
       "      <td>Juju</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>X5zBGJiy83S4M2B36Nbi8H</td>\n",
       "      <td>King Sunny Ade- Edide E Mujo.mp3</td>\n",
       "      <td>King Sunny Ade</td>\n",
       "      <td>Juju</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      id  \\\n",
       "0           0  RaLMYYE7HuG6i67bQ2t3cn   \n",
       "1           1  D7nFHkDRet5sMJ9LSsQEdf   \n",
       "2           2  zyMmRtsknzzNhFaB7kohtD   \n",
       "3           3  PHtSce72gTmGC3VHAgL7hY   \n",
       "4           4  X5zBGJiy83S4M2B36Nbi8H   \n",
       "\n",
       "                                         artist_song          artist genre  \n",
       "0                   king sunny adeSynchro System.mp3  King Sunny Ade  Juju  \n",
       "1                         nHfXJu68Yq28bT7oJbW7gA.mp3  King Sunny Ade  Juju  \n",
       "2  king Sunny Ade  Asiko Layee  Eda Kan Ko  mola ...  King Sunny Ade  Juju  \n",
       "3                       King Sunny Ade - Penkele.mp3  King Sunny Ade  Juju  \n",
       "4                   King Sunny Ade- Edide E Mujo.mp3  King Sunny Ade  Juju  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['filename'] = train['genre'] + '/' + train['artist'] + '/' + train['id'] + '.mp3'\n",
    "test['filename'] = test['genre'] + '/' + test['artist'] + '/' + test['id'] + '.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "afceb447-9a8f-4cc4-a7b2-eabc75c3f0aa",
    "_kg_hide-output": true,
    "_uuid": "dad27c6a5ef1fdad658ce710fe16fca58c75a05c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  327\n",
      "Number of testing examples:  84\n",
      "Number of classes:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \", train.shape[0])\n",
    "print(\"Number of testing examples: \", test.shape[0])\n",
    "print(\"Number of classes: \", len(train.genre.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "99b8ebbd-aa18-427a-ab33-e88553a564f6",
    "_kg_hide-output": true,
    "_uuid": "0c3e7629b5e60cfad2a7e1681dcf6e7c55c92e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Juju' 'Highlife']\n"
     ]
    }
   ],
   "source": [
    "print(train.genre.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/songs/'\n",
    "# '/Users/femi/Desktop/ML Mentorship Project/project/data/songs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42761f3d-0d20-4a97-843a-02186299f76b",
    "_uuid": "b3a730fc5ee4a9ab5904cddda84a05ac118c749d"
   },
   "source": [
    "<a id=\"audio_length\"></a>\n",
    "### Audio Length\n",
    "\n",
    "We shall now analyze the lengths of the audio files in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "40b7ba05-45df-4779-be29-b177b9b9b8e1",
    "_uuid": "867f0074922314b78de6bd9d14b308b634d1fbbe",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bab7607ffa29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolinplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"genre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# plt.xticks(rotation=90)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distribution of song durartion, per genre'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import librosa\n",
    "# train['duration'] = train['filename'].apply(lambda filename: librosa.get_duration(filename = ('data/songs/train/' + filename))/60)\n",
    "\n",
    "# _, ax = plt.subplots(figsize=(16, 4))\n",
    "# sns.violinplot(ax=ax, x=\"genre\", y=\"duration\", data=meta)\n",
    "# # plt.xticks(rotation=90)\n",
    "# plt.title('Distribution of song durartion, per genre', fontsize=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "146baca0-66cc-4ce1-8d16-16ae5764a354",
    "_uuid": "64462b38a986a2f40deeb6a053b9d99d8f6993b5"
   },
   "source": [
    "##### <a id=\"1d_model_building\"></a>\n",
    "## <center>2. Building a Model using Raw Wave</center>\n",
    "We will build two models:\n",
    "1. The first model will take the raw audio (1D array) as input and the primary operation will be Conv1D\n",
    "2. The second model will take the MFCCs as input. (We will explain MFCC later)\n",
    "\n",
    "<a id=\"1d_discription\"></a>\n",
    "### Keras Model using raw wave\n",
    "\n",
    "Our model has the architecture as follows:\n",
    "![raw](https://raw.githubusercontent.com/zaffnet/images/master/images/raw_model.jpg)\n",
    "\n",
    "**Important:**\n",
    "Due to the time limit on Kaggle Kernels, it is not possible to perform 10-fold training of a large model. I have trained the model locally and uploaded its output files as a dataset. If you wish to train the bigger model, change `COMPLETE_RUN = True` at the beginning of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ef1062c-be8a-4021-a50a-3df9bacd30fc",
    "_uuid": "2df0e6e509896eaefd30f6b4c15b55736760aafa"
   },
   "source": [
    "#### Some sssential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "58fbb75c-1ef8-478f-a5fd-3fe6cfda32af",
    "_kg_hide-output": true,
    "_uuid": "36454f818dcbe02852e7a639d428a004a387ce9f"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "from keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n",
    "from keras.utils import Sequence, to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64df4fea-4917-4762-b9be-68163f590c13",
    "_uuid": "927b4d615e24291f3c9510b653e723dc031fd042"
   },
   "source": [
    "<a id=\"configuration\"></a>\n",
    "#### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1dda9e10-5b51-430a-b20d-a319695df25d",
    "_uuid": "a9dc3968c8915e1d96f0bc011e67db26932ab0a3"
   },
   "source": [
    "The Configuration object stores those learning parameters that are shared between data generators, models, and training functions. Anything that is `global` as far as the training is concerned can become the part of Configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "e0836104-1a4d-485d-9cc1-3e5b82f449de",
    "_uuid": "66640745984135b853d36eac127fb2da302319ad"
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                 sampling_rate=16000, audio_duration=2, n_classes=2,\n",
    "                 use_mfcc=False, n_folds=5, learning_rate=0.001, \n",
    "                 max_epochs=1, n_mfcc=20):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dbdcf3fb-f915-482c-ad8f-d8578de8f080",
    "_uuid": "b1a794352ac7505abcf212d1b1c6deef32178ab3"
   },
   "source": [
    "<a id=\"data_generator\"></a>\n",
    "#### DataGenerator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "059d4658-f1a4-4d6a-ae67-05140fc9bac6",
    "_uuid": "f1a0716a545ade83970005951719e71cebe35ab2"
   },
   "source": [
    "The DataGenerator class inherits from **`keras.utils.Sequence`** . It is useful for preprocessing and feeding the data to a Keras model. \n",
    "* Once initialized with a batch_size, it computes the number of batches in an epoch. The **`__len__`** method tells Keras how many batches to draw in each epoch. \n",
    "* The **`__getitem__`** method takes an index (which is the batch number) and returns a batch of the data (both X and y) after calculating the offset. During test time, only `X` is returned.\n",
    "* If we want to perform some action after each epoch (like shuffle the data, or increase the proportion of augmented data), we can use the **`on_epoch_end`** method.\n",
    "\n",
    "Note:\n",
    "**`Sequence`** are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "f9d14e7d-89d8-42f0-9eb3-f895645b2de2",
    "_uuid": "aca30bc0f6fccf71e4b9a68e5c04c1aaf950b169"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, config, data_dir, list_IDs, labels=None, \n",
    "                 batch_size=32, preprocessing_fn=lambda x: x):\n",
    "        self.config = config\n",
    "        self.data_dir = data_dir\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocessing_fn = preprocessing_fn\n",
    "        self.on_epoch_end()\n",
    "        self.dim = self.config.dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        return self.__data_generation(list_IDs_temp)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        cur_batch_size = len(list_IDs_temp)\n",
    "        X = np.empty((cur_batch_size, *self.dim))\n",
    "\n",
    "        input_length = self.config.audio_length\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            file_path = self.data_dir + ID\n",
    "            \n",
    "            # Read and Resample the audio\n",
    "            data, _ = librosa.core.load(file_path, sr=self.config.sampling_rate,\n",
    "                                        res_type='kaiser_fast')\n",
    "\n",
    "            # Random offset / Padding\n",
    "            if len(data) > input_length:\n",
    "                max_offset = len(data) - input_length\n",
    "                offset = np.random.randint(max_offset)\n",
    "                data = data[offset:(input_length+offset)]\n",
    "            else:\n",
    "                if input_length > len(data):\n",
    "                    max_offset = input_length - len(data)\n",
    "                    offset = np.random.randint(max_offset)\n",
    "                else:\n",
    "                    offset = 0\n",
    "                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "                \n",
    "            # Normalization + Other Preprocessing\n",
    "            if self.config.use_mfcc:\n",
    "                data = librosa.feature.mfcc(data, sr=self.config.sampling_rate,\n",
    "                                                   n_mfcc=self.config.n_mfcc)\n",
    "                data = np.expand_dims(data, axis=-1)\n",
    "            else:\n",
    "                data = self.preprocessing_fn(data)[:, np.newaxis]\n",
    "            X[i,] = data\n",
    "\n",
    "        if self.labels is not None:\n",
    "            y = np.empty(cur_batch_size, dtype=int)\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                y[i] = self.labels[ID]\n",
    "            return X, to_categorical(y, num_classes=self.config.n_classes)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49a23330-291d-4eb7-aeb9-4abcfd648277",
    "_uuid": "6b69d10980c7aad004c6a7fa860c649d0b875a0f"
   },
   "source": [
    "<a id=\"1d_normalization\"></a>\n",
    "#### Normalization\n",
    "\n",
    "Normalization is a crucial preprocessing step. The simplest method is rescaling the range of features to scale the range in [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "bb5936dd-5fb1-4894-8165-6daf372a6832",
    "_uuid": "c9db10ad526815730a6e5a1f057de8c9bff12615"
   },
   "outputs": [],
   "source": [
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data-min_data)/(max_data-min_data+1e-6)\n",
    "    return data-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b9656b0-31d3-47ea-9bb3-789a40026793",
    "_uuid": "c2f0bbd810926b309d3b02473e937a2a86bc9005"
   },
   "source": [
    "* The dummy model is just for debugging purpose.\n",
    "* Our 1D Conv model is fairly deep and is trained using Adam Optimizer with a learning rate of 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "245887b3-a0dc-498d-900c-dd1c2898d955",
    "_uuid": "40771630994b93eee040c239f1c0e3bf88f13ced"
   },
   "outputs": [],
   "source": [
    "def get_1d_dummy_model(config):\n",
    "    \n",
    "    nclass = config.n_classes\n",
    "    input_length = config.audio_length\n",
    "    \n",
    "    inp = Input(shape=(input_length,1))\n",
    "    x = GlobalMaxPool1D()(inp)\n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def get_1d_conv_model(config):\n",
    "    \n",
    "    nclass = config.n_classes\n",
    "    input_length = config.audio_length\n",
    "    \n",
    "    inp = Input(shape=(input_length,1))\n",
    "    x = Convolution1D(16, 9, activation=relu, padding=\"valid\")(inp)\n",
    "    x = Convolution1D(16, 9, activation=relu, padding=\"valid\")(x)\n",
    "    x = MaxPool1D(16)(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
    "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
    "    x = MaxPool1D(4)(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
    "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
    "    x = MaxPool1D(4)(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    x = Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n",
    "    x = Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "\n",
    "    x = Dense(64, activation=relu)(x)\n",
    "    x = Dense(1028, activation=relu)(x)\n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e67aa4a-f2d0-4889-a1da-b6d3217edb5e",
    "_uuid": "32afe89ebdee366de311a6fffb5c49a0e568aaa8"
   },
   "source": [
    "<a id=\"1d_training\"></a>\n",
    "#### Training 1D Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a93de421-33be-4104-bcfa-b581cbde3d75",
    "_uuid": "ddbcf58975c5cd7436314a77e5b8f938640bcf34"
   },
   "source": [
    "It is important to convert raw labels to integer indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "e9027035-0e77-47dd-8616-113c1cfb37e0",
    "_uuid": "53aca10261dea0b8357e39adb513c7689b7c07ff"
   },
   "outputs": [],
   "source": [
    "LABELS = list(train.genre.unique())\n",
    "label_idx = {genre: i for i, genre in enumerate(LABELS)}\n",
    "train.set_index(\"filename\", inplace=True)\n",
    "test.set_index(\"filename\", inplace=True)\n",
    "train[\"label_idx\"] = train.genre.apply(lambda x: label_idx[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>artist_song</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>duration</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshCia.mp3</th>\n",
       "      <td>322</td>\n",
       "      <td>HkoSAG3PtUeynTSMeshCia</td>\n",
       "      <td>Dr Sir Warrior  His Oriental Brothers - ORIENT...</td>\n",
       "      <td>Oriental Brothers</td>\n",
       "      <td>Highlife</td>\n",
       "      <td>6.144435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4wkj.mp3</th>\n",
       "      <td>323</td>\n",
       "      <td>zus7M6hcVWvmy4ehiR4wkj</td>\n",
       "      <td>Oriental Brothers -Ama Onye Wu Onye (1).mp3</td>\n",
       "      <td>Oriental Brothers</td>\n",
       "      <td>Highlife</td>\n",
       "      <td>14.036898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCdpF.mp3</th>\n",
       "      <td>324</td>\n",
       "      <td>GJvqvLoiWqm9UXDhkVCdpF</td>\n",
       "      <td>Oriental Brothers - Nwanyi Di Ya Bu Eze.mp3</td>\n",
       "      <td>Oriental Brothers</td>\n",
       "      <td>Highlife</td>\n",
       "      <td>6.201905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiNJE.mp3</th>\n",
       "      <td>325</td>\n",
       "      <td>8SsB2ad2GR3PgPvrMDiNJE</td>\n",
       "      <td>ANAM ELE CHIE EL OLEA - Oriental Brothers Inte...</td>\n",
       "      <td>Oriental Brothers</td>\n",
       "      <td>Highlife</td>\n",
       "      <td>4.960653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wtxN.mp3</th>\n",
       "      <td>326</td>\n",
       "      <td>rdvzE6zCyRtqLeVDk7wtxN</td>\n",
       "      <td>The Oriental Brothers International  Otunwa (1...</td>\n",
       "      <td>Oriental Brothers</td>\n",
       "      <td>Highlife</td>\n",
       "      <td>6.321197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Unnamed: 0  \\\n",
       "filename                                                         \n",
       "Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshC...         322   \n",
       "Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4w...         323   \n",
       "Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCd...         324   \n",
       "Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiN...         325   \n",
       "Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wt...         326   \n",
       "\n",
       "                                                                        id  \\\n",
       "filename                                                                     \n",
       "Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshC...  HkoSAG3PtUeynTSMeshCia   \n",
       "Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4w...  zus7M6hcVWvmy4ehiR4wkj   \n",
       "Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCd...  GJvqvLoiWqm9UXDhkVCdpF   \n",
       "Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiN...  8SsB2ad2GR3PgPvrMDiNJE   \n",
       "Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wt...  rdvzE6zCyRtqLeVDk7wtxN   \n",
       "\n",
       "                                                                                          artist_song  \\\n",
       "filename                                                                                                \n",
       "Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshC...  Dr Sir Warrior  His Oriental Brothers - ORIENT...   \n",
       "Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4w...        Oriental Brothers -Ama Onye Wu Onye (1).mp3   \n",
       "Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCd...        Oriental Brothers - Nwanyi Di Ya Bu Eze.mp3   \n",
       "Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiN...  ANAM ELE CHIE EL OLEA - Oriental Brothers Inte...   \n",
       "Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wt...  The Oriental Brothers International  Otunwa (1...   \n",
       "\n",
       "                                                               artist  \\\n",
       "filename                                                                \n",
       "Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshC...  Oriental Brothers   \n",
       "Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4w...  Oriental Brothers   \n",
       "Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCd...  Oriental Brothers   \n",
       "Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiN...  Oriental Brothers   \n",
       "Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wt...  Oriental Brothers   \n",
       "\n",
       "                                                       genre   duration  \\\n",
       "filename                                                                  \n",
       "Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshC...  Highlife   6.144435   \n",
       "Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4w...  Highlife  14.036898   \n",
       "Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCd...  Highlife   6.201905   \n",
       "Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiN...  Highlife   4.960653   \n",
       "Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wt...  Highlife   6.321197   \n",
       "\n",
       "                                                    label_idx  \n",
       "filename                                                       \n",
       "Highlife/Oriental Brothers/HkoSAG3PtUeynTSMeshC...          1  \n",
       "Highlife/Oriental Brothers/zus7M6hcVWvmy4ehiR4w...          1  \n",
       "Highlife/Oriental Brothers/GJvqvLoiWqm9UXDhkVCd...          1  \n",
       "Highlife/Oriental Brothers/8SsB2ad2GR3PgPvrMDiN...          1  \n",
       "Highlife/Oriental Brothers/rdvzE6zCyRtqLeVDk7wt...          1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "f2f2dc50-77d3-43ba-bf7f-3c6b39beb67b",
    "_uuid": "604a3c7971599898b5614a67da12da84ab651a55"
   },
   "outputs": [],
   "source": [
    "config = Config(sampling_rate=16000, audio_duration=2, n_folds=3,  learning_rate=0.001)\n",
    "COMPLETE_RUN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e31b98ec-cecb-4584-9bbc-bc2748476b49",
    "_uuid": "7a2a5e44d82a2b9e04117b76464225278ec4a1d8"
   },
   "source": [
    "Here is the code for 10-fold training:\n",
    "* We use **`from sklearn.cross_validation.StratifiedKFold`** for splitting the trainig data into 10 folds.\n",
    "* We use some Keras callbacks to monitor the training.\n",
    "    * **`ModelCheckpoint`** saves the best weight of our model (using validation data). We use this weight to make test predictions.\n",
    "    * **`EarlyStopping`** stops the training once validation loss ceases to decrease\n",
    "    * **`TensorBoard`** helps us visualize training and validation loss and accuracy.\n",
    "* We fit the model using **`DataGenerator`** for training and validation splits. \n",
    "* We get both training and test predictions and save them as .npy format. We also generate a submission file. For 10-fold CV, the number of prediction files should be 10. We will ensemble these predictions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "##################################################\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 1362s 195s/step - loss: 0.9045 - acc: 0.4617 - val_loss: 0.8874 - val_acc: 0.4727\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.88742, saving model to best_0.h5\n",
      "6/6 [==============================] - 1228s 205s/step\n",
      "2/2 [==============================] - 333s 167s/step\n",
      "Fold:  1\n",
      "##################################################\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 2040s 291s/step - loss: 0.6965 - acc: 0.4524 - val_loss: 0.6946 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69459, saving model to best_1.h5\n",
      "6/6 [==============================] - 1285s 214s/step\n",
      "2/2 [==============================] - 929s 465s/step\n",
      "Fold:  2\n",
      "##################################################\n",
      "Epoch 1/1\n",
      "7/7 [==============================] - 1299s 186s/step - loss: 0.7570 - acc: 0.4611 - val_loss: 0.7498 - val_acc: 0.4722\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74985, saving model to best_2.h5\n",
      "6/6 [==============================] - 1411s 235s/step\n",
      "2/2 [==============================] - 346s 173s/step\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_FOLDER = \"predictions_1d_conv\"\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "skf = StratifiedKFold(train.label_idx, n_folds=config.n_folds)\n",
    "\n",
    "for i, (train_split, val_split) in enumerate(skf):\n",
    "    train_set = train.iloc[train_split]\n",
    "    val_set = train.iloc[val_split]\n",
    "    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "    print(\"Fold: \", i)\n",
    "    print(\"#\"*50)\n",
    "    if COMPLETE_RUN:\n",
    "        model = get_1d_conv_model(config)\n",
    "    else:\n",
    "        model = get_1d_dummy_model(config)\n",
    "\n",
    "    train_generator = DataGenerator(config, 'data/songs/train/', train_set.index, \n",
    "                                    train_set.label_idx, batch_size=32,\n",
    "                                    preprocessing_fn=audio_norm)\n",
    "    val_generator = DataGenerator(config, 'data/songs/train/', val_set.index, \n",
    "                                  val_set.label_idx, batch_size=32,\n",
    "                                  preprocessing_fn=audio_norm)\n",
    "\n",
    "    history = model.fit_generator(train_generator, callbacks=callbacks_list, validation_data=val_generator,\n",
    "                                  epochs=config.max_epochs, use_multiprocessing=True, workers=2, max_queue_size=10)\n",
    "\n",
    "    model.load_weights('best_%d.h5'%i)\n",
    "\n",
    "    # Save train predictions\n",
    "    train_generator = DataGenerator(config, 'data/songs/train/', train.index, batch_size=64,\n",
    "                                    preprocessing_fn=audio_norm)\n",
    "    predictions = model.predict_generator(train_generator, use_multiprocessing=True, \n",
    "                                          workers=2, max_queue_size=10, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    test_generator = DataGenerator(config, 'data/songs/test/', test.index, batch_size=64,\n",
    "                                    preprocessing_fn=audio_norm)\n",
    "    predictions = model.predict_generator(test_generator, use_multiprocessing=True, \n",
    "                                          workers=2, max_queue_size=10, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "#     # Make a submission file\n",
    "#     top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "#     predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "#     test['label'] = predicted_labels\n",
    "#     test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e81537d9-d886-4bd5-a923-7efe1aa1812d",
    "_kg_hide-output": true,
    "_uuid": "1e68d5ae8e431445151c8c7744fadb65fbf692c8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "488df4a9-b090-4397-a649-2e94f9ee82ad",
    "_uuid": "2afcdcf0f77f8685f57e2d119ec0cc650b7255d7"
   },
   "source": [
    "<a id=\"1d_ensembling\"></a>\n",
    "#### Ensembling 1D Conv Predictions\n",
    "Now that we have trained our model, it is time average the predictions of 10-folds. We will try Geometric Mean averaging and see what will be our Public LB score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4050aede-678b-4f9e-bb95-e70f79e4f6bd",
    "_kg_hide-output": true,
    "_uuid": "bfdddecb92be07d06e71d25b1812d064a0cee66d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for i in range(10):\n",
    "    pred_list.append(np.load(\"../input/freesound-prediction-file/test_predictions_%d.npy\"%i))\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "for pred in pred_list:\n",
    "    prediction = prediction*pred\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "# Make a submission file\n",
    "top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test = pd.read_csv('../input/freesound-audio-tagging/sample_submission.csv')\n",
    "test['label'] = predicted_labels\n",
    "test[['fname', 'label']].to_csv(\"1d_conv_ensembled_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c437de1-ecc0-4c72-9595-c689c101a72c",
    "_uuid": "40ef0374888d1453eed07c8daa18f231c12ef36d"
   },
   "source": [
    "<a id=\"intro_mfcc\"></a>\n",
    "## <center> 3. Introuction to MFCC\n",
    "\n",
    "As we have seen in the previous section, our Deep Learning models are powerful enough to classify sounds from the raw audio. We do not require any complex feature engineering. But before the Deep Learning era, people developed techniques to extract features from audio signals. It turns out that these techniques are still useful. One such technique is computing the MFCC (Mel Frquency Cepstral Coefficients) from the raw audio. Before we jump to MFCC, let's talk about extracting features from the sound.\n",
    "\n",
    "If we just want to classify some sound, we should build features that are **speaker independent**. Any feature that only gives information about the speaker (like the pitch of their voice) will not be helpful for classification. In other words, we should extract features that depend on the \"content\" of the audio rather than the nature of the speaker. Also, a good feature extraction technique should mimic the human speech perception. We don't hear loudness on a linear scale. If we want to double the perceived loudness of a sound, we have to put 8 times as much energy into it. Instead of a linear scale, our perception system uses a log scale. \n",
    "\n",
    "Taking these things into account, Davis and Mermelstein came up with MFCC in the 1980's. MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics. The underlying mathematics is quite complicated and we will skip that. For those interested, here is the [detailed explanation](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/).\n",
    "\n",
    "![http://recognize-speech.com/images/FeatureExtraction/MFCC/MFCC_Flowchart.png](http://recognize-speech.com/images/FeatureExtraction/MFCC/MFCC_Flowchart.png)\n",
    "\n",
    "<a id=\"librosa_mfcc\"></a>\n",
    "#### Generating MFCC using Librosa\n",
    "The library librosa has a function to calculate MFCC. Let's compute the MFCC of an audio file and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dcb2a6e7-b086-4d1a-94a4-215f2cb101d0",
    "_uuid": "2f8dfd08f109ababeaca9ce900b68b8a716d28b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "SAMPLE_RATE = 44100\n",
    "fname = '../input/freesound-audio-tagging/audio_train/' + '00044347.wav'   # Hi-hat\n",
    "wav, _ = librosa.core.load(fname, sr=SAMPLE_RATE)\n",
    "wav = wav[:2*44100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6250242e-e3c5-4cb9-8405-43d3279dada1",
    "_kg_hide-output": true,
    "_uuid": "7498089442d866816aabc85234a8a5546c5e58da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(wav, sr = SAMPLE_RATE, n_mfcc=40)\n",
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d02be92a-f208-42c2-ac4a-e0b2b22ba195",
    "_uuid": "f4054a6856eaa16cf82cacb5bd08ea53cdab386b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(mfcc, cmap='hot', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5015b22f-5de8-4a86-aef4-074bf90023aa",
    "_uuid": "59502f44b22674250a047e89b610867d6c6306c3"
   },
   "source": [
    "<a id=\"2d_model_building\"></a>\n",
    "## <center>4. Building a Model using MFCC\n",
    "\n",
    "We will build now build a 2D Convolutional model using MFCC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "384fe65d-fe10-4eee-826c-75c4dffcfa2d",
    "_kg_hide-output": true,
    "_uuid": "ed54039a4e0b91d10f603799feb8166404bbceec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten,\n",
    "                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97d07753-d78d-465d-936d-7f03eaf1def1",
    "_uuid": "0b2ac601f52ae4ed9dc849fcd095ab94cfe878fe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_2d_dummy_model(config):\n",
    "    \n",
    "    nclass = config.n_classes\n",
    "    \n",
    "    inp = Input(shape=(config.dim[0],config.dim[1],1))\n",
    "    x = GlobalMaxPool2D()(inp)\n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_2d_conv_model(config):\n",
    "    \n",
    "    nclass = config.n_classes\n",
    "    \n",
    "    inp = Input(shape=(config.dim[0],config.dim[1],1))\n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0c823de-9971-4247-9501-dc74d2f95d8e",
    "_uuid": "d88e90fdc36c77c10ecc8f674d6fd39c8e4d78fb"
   },
   "source": [
    "<a id=\"2d_data\"></a>\n",
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb5aef7d-669b-4cde-9e09-a2bfaa379cc9",
    "_uuid": "70b8cd145ae3838c7974fe257403c8c7fbc8552a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config(sampling_rate=44100, audio_duration=2, n_folds=10, \n",
    "                learning_rate=0.001, use_mfcc=True, n_mfcc=40)\n",
    "if not COMPLETE_RUN:\n",
    "    config = Config(sampling_rate=44100, audio_duration=2, n_folds=2, \n",
    "                    max_epochs=1, use_mfcc=True, n_mfcc=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b9b1c9b-7e02-46f3-96f6-67ebc9bf9132",
    "_uuid": "5242e943f1bc1154d19c03c361a826553c811cfe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, config, data_dir):\n",
    "    X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))\n",
    "    input_length = config.audio_length\n",
    "    for i, fname in enumerate(df.index):\n",
    "        print(fname)\n",
    "        file_path = data_dir + fname\n",
    "        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "\n",
    "        data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n",
    "        data = np.expand_dims(data, axis=-1)\n",
    "        X[i,] = data\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c9910de1-388b-470e-8908-6df548f1b866",
    "_uuid": "bb3bc487b52a549a856807dd838a4f6cd209917d"
   },
   "source": [
    "```python\n",
    "X_train = prepare_data(train, config, '../input/freesound-audio-tagging/audio_train/')\n",
    "X_test = prepare_data(test, config, '../input/freesound-audio-tagging/audio_test/')\n",
    "y_train = to_categorical(train.label_idx, num_classes=config.n_classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0e0b17b-d2f8-47f8-9b4d-fff3b2761dde",
    "_uuid": "89e8bd3dc6d1f432309e668685fb98d1ce866e95"
   },
   "source": [
    "<a id=\"2d_normalization\"></a>\n",
    "#### Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "928b0993-7799-4b75-bef8-c1df3755632e",
    "_uuid": "60b6d9dfcb25eb9b3cef7e05675d67b104e24b31"
   },
   "source": [
    "```python\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af8afd09-66bf-4618-ad95-d70db35b90ec",
    "_uuid": "b70fea949114595111c39f9f64fb1752603e3fdf"
   },
   "source": [
    "<a id=\"2d_training\"></a>\n",
    "#### Training 2D Conv on MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10fb7477-8122-49fb-be81-1adb7aa45c7f",
    "_uuid": "ab0f2bc7e7bbaced3eb2e4c3acb7c7c63aa73681"
   },
   "source": [
    "```python\n",
    "PREDICTION_FOLDER = \"predictions_2d_conv\"\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "skf = StratifiedKFold(train.label_idx, n_folds=config.n_folds)\n",
    "for i, (train_split, val_split) in enumerate(skf):\n",
    "    K.clear_session()\n",
    "    X, y, X_val, y_val = X_train[train_split], y_train[train_split], X_train[val_split], y_train[val_split]\n",
    "    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%i'%i, write_graph=True)\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "    print(\"#\"*50)\n",
    "    print(\"Fold: \", i)\n",
    "    model = get_2d_conv_model(config)\n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=callbacks_list, \n",
    "                        batch_size=64, epochs=config.max_epochs)\n",
    "    model.load_weights('best_%d.h5'%i)\n",
    "\n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=64, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=64, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bd794b7-c09e-42d6-8f8a-158758921273",
    "_uuid": "b4421687f65fd8068c04fcdfdb419bf4f08c5f2c"
   },
   "source": [
    "<a id=\"2d_ensembling\"></a>\n",
    "#### Ensembling 2D Conv Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c253178-6cde-4bad-835d-d09484f381ed",
    "_uuid": "e6868eb538b9fed874fcb02183d2edd348d38b5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for i in range(10):\n",
    "    pred_list.append(np.load(\"../input/freesound-prediction-data-2d-conv-reduced-lr/test_predictions_%d.npy\"%i))\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "for pred in pred_list:\n",
    "    prediction = prediction*pred\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "# Make a submission file\n",
    "top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test = pd.read_csv('../input/freesound-audio-tagging/sample_submission.csv')\n",
    "test['label'] = predicted_labels\n",
    "test[['fname', 'label']].to_csv(\"2d_conv_ensembled_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b67760f2-f8cd-498a-b340-4910d8c443d3",
    "_uuid": "38feef2350dfa3c099bd6fb2e1a0b921716606a8"
   },
   "source": [
    "<a id=\"1d_2d_ensembling\"></a>\n",
    "## <center>5. Ensembling 1D Conv and 2D Conv Predictions</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "12566257-72a5-4aa3-9e11-763c98489810",
    "_uuid": "448e8f9034d9d43a4642b1f441965b272425ba63",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for i in range(10):\n",
    "    pred_list.append(np.load(\"../input/freesound-prediction-data-2d-conv-reduced-lr/test_predictions_%d.npy\"%i))\n",
    "for i in range(10):\n",
    "    pred_list.append(np.load(\"../input/freesound-prediction-file/test_predictions_%d.npy\"%i))\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "for pred in pred_list:\n",
    "    prediction = prediction*pred\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "# Make a submission file\n",
    "top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test = pd.read_csv('../input/freesound-audio-tagging/sample_submission.csv')\n",
    "test['label'] = predicted_labels\n",
    "test[['fname', 'label']].to_csv(\"1d_2d_ensembled_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34e4b1a5-49ac-49ff-a9cd-9b341d784e9b",
    "_uuid": "836b4a9008b4a239f3c41d8f3997bd49fa3c2280"
   },
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## <center>Results and Conclusion</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f6bb71f-f461-414c-90d1-725c03f368ff",
    "_uuid": "01cfbfa163adb3da8b7b7da3310fb9e38ce0d478"
   },
   "source": [
    "So far, we have trained two models. Let's analyze their relative complexity and strength.\n",
    "\n",
    "\n",
    "\n",
    "| Model        | Number of Trainable parameters           | Public LB score  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| 1D Conv on Raw wave      | 360,513 | 0.809 |\n",
    "| 2D Conv on MFCC (verified labels only)    | 168,361  |   0.785 |\n",
    "| 2D Conv on MFCC     | 168,361  |   0.844 |\n",
    "| 1D Conv + 2D Conv Ensemble     | N/A  |   0.895 |\n",
    "\n",
    "**As we can see, 2D Convolution on MFCC performs better than 1D Convolution on Raw waves.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78397951-3d15-45c1-bc29-17c853d7adf5",
    "_uuid": "b8db90dc506d23d6241c606ca03fd1762f38f36f"
   },
   "source": [
    "## Coming Soon\n",
    "\n",
    "1. Data Augmentation\n",
    "2. Training on Manually Verified Labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
